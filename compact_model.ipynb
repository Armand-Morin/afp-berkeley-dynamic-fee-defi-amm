{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "def zscore(series):\n",
    "    return (series - series.mean()) / series.std()\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid(x, k=1, x0=0):\n",
    "    return 1 / (1 + np.exp(-k * (x - x0)))\n",
    "\n",
    "\n",
    "def adjusted_sigmoid(x, f_min=0.01, f_max=0.1, k=1, x0=0):\n",
    "    return f_min + (f_max - f_min) * sigmoid(x, k=k, x0=x0)\n",
    "\n",
    "\n",
    "def global_sigmoid(x, f_min, f_max, k=1, x0=0):\n",
    "    if x > 0:\n",
    "        return adjusted_sigmoid(x, f_min, f_max, k, x0)\n",
    "    else:\n",
    "        return adjusted_sigmoid(-x, f_min, f_max, k, x0)\n",
    "    \n",
    "    \n",
    "def calculate_markout(d, v, f, p):\n",
    "    m = d * v * (f - p)\n",
    "    return m\n",
    "\n",
    "\n",
    "data = pd.read_pickle('data/Merged_CEX_DEX_v2_p1.pkl')\n",
    "data['time'] = pd.to_datetime(data['time'])\n",
    "data = data.sort_values('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['impermanent_loss'] = (2 * np.sqrt(data['price_dex'].shift(1) / data['price_dex']) / (1 + data['price_dex'].shift(1) / data['price_dex']) - 1).fillna(0)\n",
    "data['LVR_nbtoken']= data['LVR']/data['price_dex']\n",
    "data['avg_gas']    = data['gasUsed'] * data['gasPrice_eth'].rolling(100).mean() * data['price_dex']\n",
    "data['arb_fee']    = (np.abs(data['LVR_clean'] * data['amountUSD']) - (data['tcost_usd'] - data['avg_gas']))*100 / data['amountUSD']\n",
    "data['LVR_zscore'] = zscore(data['LVR'])\n",
    "data['std']             = data['price_dex'].rolling(100).std()\n",
    "data['mean_rol_amount'] = data['amountUSD'].rolling(10).mean()\n",
    "data['rol_amountUSD']   = data['amountUSD'].rolling(100).sum()\n",
    "\n",
    "# can be seen as looking ahead but in a DEX we can compute it aheat with the liquidity and the amount\n",
    "data['price_impact']    = data['price_dex'].pct_change(1).shift(-1)*100\n",
    "\n",
    "data['time_diff']           = data['time'].diff().dt.total_seconds().fillna(0)\n",
    "data['rolling_time_span']   = data['time_diff'].rolling(window=3600, min_periods=1).sum()\n",
    "data['rolling_trade_count'] = data['time'].rolling(window=3600, min_periods=1).count()\n",
    "data['lambda'] = data['rolling_trade_count'] / data['rolling_time_span']\n",
    "data['lambda'].replace([float('inf'), float('-inf'), pd.NA], 0, inplace=True)\n",
    "data = data.drop(columns=['time_diff'])\n",
    "\n",
    "data['target_fee'] = -np.tanh(np.sign(data['amount1']) * data['arb_fee']/100)\n",
    "\n",
    "data = data.dropna()\n",
    "\n",
    "features = ['LVR_nbtoken', 'LVR', 'std', 'mean_rol_amount', 'avg_gas', 'price_impact', 'rol_amountUSD', 'lambda',  'rolling_time_span', 'amountUSD']\n",
    "target = 'tcost_usd'\n",
    "\n",
    "lower_bound = data[features].quantile(0.10)\n",
    "upper_bound = data[features].quantile(0.90)\n",
    "filtered_indices = data[features].apply(lambda x: x.between(lower_bound[x.name], upper_bound[x.name])).all(axis=1)\n",
    "data_filtered = data.loc[filtered_indices].copy()\n",
    "\n",
    "f_min = 0.01\n",
    "f_max = 0.1\n",
    "w     = 0.04\n",
    "k     = 1.75  # Steepness of the curve\n",
    "\n",
    "params = {}\n",
    "for f in features:\n",
    "    params[f] = {'f_min': f_min, 'f_max': f_max, 'k': k, 'x0': (f_max-f_min)/(2*w)}\n",
    "    \n",
    "    \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_filtered = data_filtered[features]\n",
    "y_filtered = data_filtered[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_filtered, y_filtered, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "feature_importances = model.feature_importances_ / model.feature_importances_.sum()\n",
    "features_importance_normalized = {feature: importance for feature, importance in zip(features, feature_importances)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_subset = data_filtered[data_filtered['amount1'] < 0].copy()\n",
    "sell_subset = data_filtered[data_filtered['amount1'] > 0].copy()\n",
    "\n",
    "for f in features:    \n",
    "    # here for each features the parameters of the sigmoid will be the one to calibrate\n",
    "    buy_subset[f'{f}_fee'] = buy_subset[f].apply(lambda x: adjusted_sigmoid(x, params[f]['f_min'], params[f]['f_max'], params[f]['k'], x0))\n",
    "    sell_subset[f'{f}_fee'] = -sell_subset[f].apply(lambda x: adjusted_sigmoid(-x, params[f]['f_min'], params[f]['f_max'], k, x0))\n",
    "\n",
    "combined_data = pd.concat([buy_subset, sell_subset])\n",
    "\n",
    "# Apply feature importances as weights\n",
    "for f in features:\n",
    "    weight = features_importance_normalized[f]\n",
    "    combined_data[f'{f}_weighted_fee'] = combined_data[f'{f}_fee'] * weight\n",
    "\n",
    "# sum weighted fees to get a combined fee\n",
    "weighted_fee_columns = [f'{f}_weighted_fee' for f in features]\n",
    "combined_data['combined_fee'] = combined_data[weighted_fee_columns].sum(axis=1)\n",
    "combined_data['target_fee_usd'] = combined_data['target_fee'] * combined_data['amount0']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = combined_data.resample('1Min', on='time_dex').agg({\n",
    "    'price_dex'           : 'ohlc',\n",
    "    'amount0'             : 'sum',\n",
    "    'amount1'             : 'sum',\n",
    "    'amountUSD'           : 'sum',\n",
    "    'LVR'                 : 'sum',\n",
    "    'impermanent_loss'    : 'sum',\n",
    "    'tcost_usd'           : 'sum',\n",
    "    'target_fee_usd'      : 'sum',\n",
    "    'combined_fee'        : 'sum',\n",
    "})\n",
    "metrics.columns = metrics.columns.droplevel(0)\n",
    "metrics = metrics.rename(columns={'amountUSD': 'volumeUSD'})\n",
    "metrics['1m_volumeUSD'] = metrics['volumeUSD'].rolling(window=60).sum()\n",
    "\n",
    "metrics['future_close'] = metrics['close'].shift(-60)\n",
    "metrics = metrics.dropna()\n",
    "\n",
    "metrics['markout'] = metrics.apply(lambda x: calculate_markout(d=np.sign(x['amount0']), v=x['volumeUSD'], f=x['future_close'], p=x['close']), axis=1)\n",
    "\n",
    "pool_fee_rate = 0.05/100\n",
    "\n",
    "metrics['trade_direction'] = np.sign(metrics['amount0'])\n",
    "metrics['cfee'] = pool_fee_rate * metrics['volumeUSD']\n",
    "metrics['dfee'] = metrics['combined_fee'] * metrics['volumeUSD']\n",
    "\n",
    "metrics['cpnl'] = metrics['close'].shift(-1) - metrics['close'] + metrics['cfee'].abs()\n",
    "metrics['dpnl'] = metrics['close'].shift(-1) - metrics['close'] + metrics['dfee'].abs()\n",
    "\n",
    "metrics['fees_cumul'] = (metrics['volumeUSD'] * pool_fee_rate).cumsum()\n",
    "metrics['avg_7m_volume'] = (metrics['volumeUSD'] * pool_fee_rate).rolling(window=7).mean()\n",
    "\n",
    "metrics['cpnl_5m_positive'] = metrics['cpnl'].rolling(window=5).apply(lambda x: np.sum(x[x>0]))\n",
    "metrics['cpnl_5m_negative'] = metrics['cpnl'].rolling(window=5).apply(lambda x: np.sum(x[x<0]))\n",
    "# Calculate a 7 min ma of percent toxic flow:\n",
    "#metrics['c%_toxic_flow_ma_7m'] = (metrics['cpnl_5m_negative'] / (metrics['cpnl_5m_negative'] + metrics['cpnl_5m_positive'])).rolling(window=7).mean()\n",
    "\n",
    "metrics['dpnl_5m_positive'] = metrics['dpnl'].rolling(window=5).apply(lambda x: np.sum(x[x>0]))\n",
    "metrics['dpnl_5m_negative'] = metrics['dpnl'].rolling(window=5).apply(lambda x: np.sum(x[x<0]))\n",
    "# Calculate a 7 min ma of percent toxic flow:\n",
    "#metrics['d%_toxic_flow_ma_7m'] = (metrics['dpnl_5m_negative'] / (metrics['dpnl_5m_negative'] + metrics['dpnl_5m_positive'])).rolling(window=7).mean()\n",
    "\n",
    "metrics.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics['d%_toxic_flow_ma_7m'].sum()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
